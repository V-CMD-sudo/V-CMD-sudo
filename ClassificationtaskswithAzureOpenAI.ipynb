{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        },
        "colab": {
            "provenance": []
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<center>\n",
                "\n",
                "![image.png](data:image/png;base64,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)\n",
                "</center>"
            ],
            "metadata": {
                "id": "DoixBYLEpZ76",
                "azdata_cell_guid": "db617467-43d7-4fb9-a769-519c1c7d1203"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "<center>\n",
                "<h1><b>Classification tasks with Azure Open AI</b>\n",
                "</center>"
            ],
            "metadata": {
                "id": "zJLtsQobpd0R",
                "azdata_cell_guid": "46697e41-3c58-43d1-ad9a-56611f2b92b4"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Introduction\n",
                "\n",
                "Welcome! In this MLS, we are going to look into the E-commerce aggregators business. Aggregator platforms face multiple challenges due to their unorganised nature. One such problem is the task of categorising products. Since the sellers on the platform aren't organised, they tend to mislabel some of their products. This leads to confusion in the customers leading to dissatisfaction and ultimately revenue loss. The management had tried to educate the sellers on labelling through hand-outs however, that hasn't been successfull. So they are looking to automate labeling using AI.\n",
                "\n",
                "**Task 1: Auto-Labelling System**\n",
                "\n",
                "Initially, our focus is on categories with the highest incidence of mislabeling. Currently, we are facing a 27% mislabeling in the system. One of the reasons this is happening is because there are a lot of common words between the skin care and hair care categories. Both of them have words like oil, powder, wash, etc. It is also confusing as sometimes products for body hair can be categorised as skin care and other times as hair care. Similarly, products for scalp can be basketed into skin care or hair care.Our job is to reduce this as much as possible.\n",
                "\n",
                "1. Skin Care\n",
                "2. Hair Care"
            ],
            "metadata": {
                "id": "w_lddF6ADKJ5",
                "azdata_cell_guid": "1f06d5cb-8efe-4827-a6c1-116f47da68a0"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Task 2: Customer Intent Analysis**\n",
                "\n",
                "Management also has received concerns about the usability of the website. They aim to enhance the user experience on our platform by gaining insights into the problems customers are facing. One good source of understanding where customers are having trouble is by looking at the concerns raised by customers with call support. Understanding customer intent in chat support messages will inform the management where improvements are needed in the platform's UI/UX, making navigation and usage more intuitive.\n",
                "\n",
                "The data science team is entrusted with classifying customer intent into the relevant categories.\n",
                "\n",
                "Due to resource constraints, we must utilize a small dataset for model training.\n"
            ],
            "metadata": {
                "id": "kNCeIToJPahi",
                "azdata_cell_guid": "8a1fbdab-3895-4675-8e89-24a2ca6691bf"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's start by setting up our code environment."
            ],
            "metadata": {
                "id": "foi0sYHaQm1A",
                "azdata_cell_guid": "292af8f8-a88a-45b9-907d-2065206c63e0"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Setup"
            ],
            "metadata": {
                "id": "0zTC8qPRPahk",
                "azdata_cell_guid": "472ada41-68f8-4d1a-ac2a-5cc3860e9016"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Installation"
            ],
            "metadata": {
                "id": "ykLYC3zzSr8z",
                "azdata_cell_guid": "58a25803-bad2-46e7-bcd1-a2806e596c36"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install openai==1.2.0 tiktoken datasets session-info --quiet"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "9b23OfNvStm-",
                "outputId": "f646b324-f132-4cbf-9fbc-8a3da6ca9e9f",
                "azdata_cell_guid": "fa4ca472-bdd1-43d9-aa66-16a4912370d2",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Imports"
            ],
            "metadata": {
                "id": "YFZj9kr8Pahl",
                "azdata_cell_guid": "ccaac3ac-4c1a-4241-96f4-78230a73ef17"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Import all Python packages required to access the Azure Open AI API.\n",
                "# Import additional packages required to access datasets and create examples.\n",
                "\n",
                "import json\n",
                "import random\n",
                "import tiktoken\n",
                "import session_info\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "from openai import AzureOpenAI\n",
                "\n",
                "from datasets import load_dataset\n",
                "from collections import Counter\n",
                "from tqdm import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score"
            ],
            "metadata": {
                "id": "Q3gwxSqQPahl",
                "azdata_cell_guid": "19abc341-6763-4ca2-87bc-779ccaa64c89",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Authentication"
            ],
            "metadata": {
                "id": "SN0wIDUjPahn",
                "azdata_cell_guid": "1c96ad8e-9e90-43ca-8a08-4f08f9a7ad2d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "with open('config_v1_2.json', 'r') as az_creds:\n",
                "    data = az_creds.read()"
            ],
            "metadata": {
                "id": "cxLhZIviPahn",
                "azdata_cell_guid": "60a00636-b120-4946-9028-81e2d5d339b2",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "creds = json.loads(data)"
            ],
            "metadata": {
                "id": "nL9GpBEVPahn",
                "azdata_cell_guid": "0fe596cc-56d4-462e-bc74-a6b9a14a70d5",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "client = AzureOpenAI(\n",
                "    azure_endpoint=creds[\"AZURE_OPENAI_ENDPOINT\"],\n",
                "    api_key=creds[\"AZURE_OPENAI_KEY\"],\n",
                "    api_version=creds[\"AZURE_OPENAI_APIVERSION\"]\n",
                ")"
            ],
            "metadata": {
                "id": "pSaJqJC0YyDK",
                "azdata_cell_guid": "87ea8a51-4ab5-4544-9018-147ec514af7c",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "deployment_name = creds[\"CHATGPT_MODEL\"]"
            ],
            "metadata": {
                "id": "Atf3l2mNPahp",
                "azdata_cell_guid": "4cf4ee39-46b2-4c6d-a714-b34617d2fcaa",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Utilities"
            ],
            "metadata": {
                "id": "HQpkd5elTMhd",
                "azdata_cell_guid": "8add4188-246f-4e95-a347-d81ae9771004"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "While developing the solution, we need to be mindful of the costs it will incurr for the business. Even a good solution that comes at a high cost is not useful for the business. For LLMs, costs are associated with the number of tokens consumed. Let's create a function using tiktoken to understand the number of tokens we are using in each of out prompts. This information will be cruicial while deciding the final technique we are going to use to solve the problem."
            ],
            "metadata": {
                "id": "QZixg_xITN8s",
                "azdata_cell_guid": "f13674cc-d3d4-481b-839d-8b247a3782ef"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def num_tokens_from_messages(messages):\n",
                "\n",
                "    \"\"\"\n",
                "    Return the number of tokens used by a list of messages.\n",
                "    Adapted from the Open AI cookbook token counter\n",
                "    \"\"\"\n",
                "\n",
                "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
                "\n",
                "    # Each message is sandwiched with <|start|>role and <|end|>\n",
                "    # Hence, messages look like: <|start|>system or user or assistant{message}<|end|>\n",
                "\n",
                "    tokens_per_message = 3 # token1:<|start|>, token2:system(or user or assistant), token3:<|end|>\n",
                "\n",
                "    num_tokens = 0\n",
                "\n",
                "    for message in messages:\n",
                "        num_tokens += tokens_per_message\n",
                "        for key, value in message.items():\n",
                "            num_tokens += len(encoding.encode(value))\n",
                "\n",
                "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
                "\n",
                "    return num_tokens"
            ],
            "metadata": {
                "id": "V5Gp-_CxPahp",
                "azdata_cell_guid": "51b8ca67-198c-4df2-89e5-2a2a3e873e5d",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Task 1: Auto-Label Classificaation"
            ],
            "metadata": {
                "id": "3sP7s5TDPahq",
                "azdata_cell_guid": "b6609039-a30d-48a0-af11-d7ff1ca5998d"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's have a look at the data and get a feel of it."
            ],
            "metadata": {
                "id": "4NkdNjY3DdgD",
                "azdata_cell_guid": "9de7b926-8f60-4f68-969a-777d8c22bb11"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Preparing Data"
            ],
            "metadata": {
                "id": "wH4wcUNaR83t",
                "azdata_cell_guid": "2e20f2f5-50c3-40fc-84b0-d9b35b1d349e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "data = pd.read_csv(\"auto-labelling.csv\")"
            ],
            "metadata": {
                "id": "qzvme58ISAk6",
                "azdata_cell_guid": "e8acf6d8-9f76-4f5b-90a3-dd2c80548e55",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "data.head(10)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 363
                },
                "id": "oeSQHKNkVoiC",
                "outputId": "f3d0bb44-628a-4c32-b622-c1c1cb550d4a",
                "azdata_cell_guid": "daa7c3e3-3770-4aa1-8f15-08fb38b371ff",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "data.Category.value_counts()"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Gd7GgpUt9AE4",
                "outputId": "c3842305-71cb-4e1f-dd7a-afbbd6b9ae5c",
                "azdata_cell_guid": "730e850b-10a2-480d-8197-0632610e749e",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Note how the dataset is evenly balanced with equal number of reviews assembled for each of the category. This makes our life easy."
            ],
            "metadata": {
                "id": "pnwTt-Jt3pp5",
                "azdata_cell_guid": "d27a3971-231c-426e-90f0-f60c3089df1c"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Since this is a classification exercise with a balanced dataset, we can use accuracy as our metric. We need to also be mindful of the tokens consumed for each prompt as this is going to be a perpetual task for the business as new products are added everyday."
            ],
            "metadata": {
                "id": "hPHHq5_fImow",
                "azdata_cell_guid": "f8bab93f-e9c9-4a46-a2df-96c753ee1f6f"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Test and Train Split"
            ],
            "metadata": {
                "id": "Y8uqAOkAJEOt",
                "azdata_cell_guid": "660aa5aa-0721-483d-bfbb-685e8189d3ad"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let us split the data into two segments - one segment that gives us a pool to draw few-shot examples from and another segment that gives us a pool of gold examples which will be used for testing."
            ],
            "metadata": {
                "id": "NoDGl2mc4UrU",
                "azdata_cell_guid": "aa6021ca-f2be-4511-a00c-c41587ead681"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "In summary, we extract a dataset from a corpus by processing required fields. Each example should contain the text input and an annotated label. Once we create examples and gold examples from this dataset, this curated dataset is stored in a format appropriate for reuse (e.g., JSON)."
            ],
            "metadata": {
                "id": "uOOoD0Skm8d2",
                "azdata_cell_guid": "eebe3b93-0e02-4be4-b21f-ca7ce969b275"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "To select gold examples for this session, we sample randomly from the test data using a `random_state=42`. This ensures that the examples from multiple runs of the sampling are the same (i.e., they are randomly selected but do not change between different runs of the notebook). Note that we are doing this only to keep execution times low for illustration. In practise, large number of gold examples facilitate robust estimates of model accuracy."
            ],
            "metadata": {
                "id": "2ExVGFPl4tdx",
                "azdata_cell_guid": "3e04fcde-82ad-4bed-be0e-39c56885cd02"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "examples_df, gold_examples_df = train_test_split(\n",
                "    data, #<- the full dataset\n",
                "    test_size=0.8, #<- 80% random sample selected for gold examples\n",
                "    random_state=42, #<- ensures that the splits are the same for every session\n",
                "    stratify=data['Category'] #<- ensures equal distribution of labels\n",
                ")"
            ],
            "metadata": {
                "id": "dkXtXbc9N5rC",
                "azdata_cell_guid": "695bcaf6-3507-4309-bdf0-28bd5b5c1a97",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "gold_examples = (\n",
                "        gold_examples_df.to_json(orient='records')\n",
                ")"
            ],
            "metadata": {
                "id": "UpeZL2989746",
                "azdata_cell_guid": "531cd4f4-2875-41f6-b612-38917c0b9a58",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "(examples_df.shape, gold_examples_df.shape)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "bGSj2dfZOMW-",
                "outputId": "a03932bd-7222-4a0d-a991-9bb3ea1b6004",
                "azdata_cell_guid": "8e14cc8e-53df-4b51-ad68-8140720cd322",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "gold_examples_df.head(3)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 143
                },
                "id": "r8J58KMMM95s",
                "outputId": "dca725f6-a267-46fa-8424-11fb2bccaa0a",
                "azdata_cell_guid": "643c19d6-c5c3-4230-b855-98c1cd9d3bfe",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "With everything setup, let's start working on our prompts."
            ],
            "metadata": {
                "id": "RHcvbwDcJKZF",
                "azdata_cell_guid": "0c44d999-29b1-4b0b-bbf2-392a174b1023"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Step 3: Derive Prompt"
            ],
            "metadata": {
                "id": "ZPsU-h8FPaht",
                "azdata_cell_guid": "960b2769-cb87-442a-a4ab-7c00e9b19328"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Create prompts"
            ],
            "metadata": {
                "id": "X3uS8NWfNqBG",
                "azdata_cell_guid": "720fa836-b9e7-483f-b14d-390b3516991b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "user_message_template = \"\"\"```{product_description}```\"\"\""
            ],
            "metadata": {
                "id": "NfDUKbCgPahu",
                "azdata_cell_guid": "1b227a3a-e039-47fc-b522-53130399e74a",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's create a zero-shot prompt for this scenario. We need to make sure that LLM outputs only the category label and not explanation. So, let's add explicit instructions for that."
            ],
            "metadata": {
                "id": "Sxwq7qwrwLwn",
                "azdata_cell_guid": "db8beff3-c1a1-4cde-9f38-bd936f6ed23c"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Prompt 1: Zero-shot**"
            ],
            "metadata": {
                "id": "w8RoXc0qJW0l",
                "azdata_cell_guid": "0fa669a6-9f11-4687-b5e5-a2a1195db131"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "zero_shot_system_message = \"\"\"\n",
                "Classify the following product desciption presented in the input into one of the following categories.\n",
                "Categories - ['Hair Care', 'Skin Care']\n",
                "Product description will be delimited by triple backticks in the input.\n",
                "Answer only 'Hair Care' or 'Skin Care'. Nothing Else. Do not explain your answer.\n",
                "\"\"\""
            ],
            "metadata": {
                "id": "J79_IaxHJdY2",
                "azdata_cell_guid": "a3d637bd-6efd-4bf0-9e31-543006874098",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "zero_shot_prompt = [{'role':'system', 'content': zero_shot_system_message}]"
            ],
            "metadata": {
                "id": "t8cjNjiJJpzc",
                "azdata_cell_guid": "fb563136-b073-4b11-84e7-fe344b22ff8b",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's check the number of tokens this prompt consumes."
            ],
            "metadata": {
                "id": "viKiMpevxVbC",
                "azdata_cell_guid": "a32955b9-b24d-4d15-84d9-ec63d96c993a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "num_tokens_from_messages(zero_shot_prompt)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "xlbBt1u_OJEz",
                "outputId": "6f464e01-6d05-4e5a-c8f3-448703e3bfe4",
                "azdata_cell_guid": "dae71fe2-d603-435c-80b9-256976fc2f49",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Let's try our zero-shot prompt on a single example.**"
            ],
            "metadata": {
                "id": "_ubtoVTCIKUm",
                "azdata_cell_guid": "d7281da9-90a8-4dc2-af7c-a63469274591"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "data.iloc[0,:]"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "tT2xnIrNJS-9",
                "outputId": "f4ec120f-8435-4050-9727-9a6b51f73486",
                "azdata_cell_guid": "8d856f0b-4fa4-4d34-8240-47c844042e6b",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "input_description = data.iloc[0,0]\n",
                "\n",
                "user_input = [\n",
                "    {\n",
                "        'role':'user',\n",
                "        'content': user_message_template.format(product_description = input_description)\n",
                "    }\n",
                "]"
            ],
            "metadata": {
                "id": "RfjpbkGgIr4e",
                "azdata_cell_guid": "ed635d59-77dd-4bf4-8126-cd818eee9ef4",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's also cap the max_token parameter to 4 so that the model doesn't output explanations. We are capping it at 4 instead of 2 because we want to leave a little lee-way for punctuation marks and sub-words token that the model might output in the middle of the text. It is better to use regex later than to prematurely over-constrain the LLM output."
            ],
            "metadata": {
                "id": "oNZm6BWTyWzx",
                "azdata_cell_guid": "0200c053-f2fe-490f-b1b9-f07579893612"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "response = client.chat.completions.create(\n",
                "    model=deployment_name,\n",
                "    messages=zero_shot_prompt+user_input,\n",
                "    temperature=0, # <- Note the low temperature\n",
                "    max_tokens=4 # <- Note how we restrict the output to not more than 2 tokens\n",
                ")\n",
                "print(response.choices[0].message.content)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "p9aCct3gIP2d",
                "outputId": "aae272a1-ef1a-4d34-e8d3-3de84c71d04d",
                "azdata_cell_guid": "05b1ffe4-3274-4a57-b0ee-92bb8f050bd3",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Great! That's a hit. Let's scale it. Let's create a generic evaluation function that can be used with all the prompting techniques that we are going to use."
            ],
            "metadata": {
                "id": "6dINbtNsKeYh",
                "azdata_cell_guid": "9ac264e6-55fd-46dd-9d73-60c85a5aa473"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def evaluate_prompt(prompt, gold_examples, user_message_template,samples_to_output = 10):\n",
                "\n",
                "    \"\"\"\n",
                "    Return the accuracy score for predictions on gold examples.\n",
                "    For each example, we make a prediction using the prompt. Gold labels and\n",
                "    model predictions are aggregated into lists and compared to compute the\n",
                "    accuracy.\n",
                "\n",
                "    Args:\n",
                "        prompt (List): list of messages in the Open AI prompt format\n",
                "        gold_examples (str): JSON string with list of gold examples\n",
                "        user_message_template (str): string with a placeholder for product description\n",
                "        samples_to_output (int): number of sample predictions and ground truths to print\n",
                "\n",
                "    Output:\n",
                "        accuracy (float): Accuracy computed by comparing model predictions\n",
                "                                with ground truth\n",
                "    \"\"\"\n",
                "\n",
                "    count =0\n",
                "    model_predictions, ground_truths = [], []\n",
                "\n",
                "    for example in json.loads(gold_examples):\n",
                "        gold_input = example['Product Description']\n",
                "        user_input = [\n",
                "            {\n",
                "                'role':'user',\n",
                "                'content': user_message_template.format(product_description=gold_input)\n",
                "            }\n",
                "        ]\n",
                "\n",
                "        try:\n",
                "            response = client.chat.completions.create(\n",
                "                model=deployment_name,\n",
                "                messages=prompt+user_input,\n",
                "                temperature=0, # <- Note the low temperature\n",
                "                max_tokens=4 # <- Note how we restrict the output to not more than 4 tokens\n",
                "            )\n",
                "\n",
                "            prediction = response.choices[0].message.content\n",
                "            # print(prediction) #uncomment to see LLM response or to debug\n",
                "            model_predictions.append(prediction.strip().lower()) # <- removes extraneous white space and lowercases output\n",
                "            ground_truths.append(example['Category'].strip().lower())\n",
                "\n",
                "            if count < samples_to_output:\n",
                "              count += 1\n",
                "              print(\"Product Description: \\n\", example['Product Description'],\"\\n\")\n",
                "              print(\"Original label: \\n\", example['Category'],\"\\n\")\n",
                "              print(\"Predicted label: \\n\", prediction)\n",
                "              print(\"====================================================\")\n",
                "\n",
                "        except Exception as e:\n",
                "            print(e)\n",
                "            continue\n",
                "\n",
                "        accuracy = accuracy_score(ground_truths, model_predictions)\n",
                "\n",
                "    return accuracy"
            ],
            "metadata": {
                "id": "tnwtFBZQoSad",
                "azdata_cell_guid": "b9f73ecd-a899-4f26-9ac4-845a2f8b129b",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "evaluate_prompt(zero_shot_prompt, gold_examples, user_message_template)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "EAaZaWBxLIYl",
                "outputId": "a404e194-9d20-4767-f9c8-5c51f720bf7f",
                "azdata_cell_guid": "57317bf5-bbca-402a-b141-58aabb96cf2d",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Decent start. Now, let's check if few-shot can do a better job."
            ],
            "metadata": {
                "id": "XRD-4UUQLKIY",
                "azdata_cell_guid": "d048647a-4416-43c6-9ec6-f7233824c6ce"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Prompt 2: Few-shot**"
            ],
            "metadata": {
                "id": "46AWalvrLTxs",
                "azdata_cell_guid": "c5ce782d-78f0-4177-a125-68fccbbc4313"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "For the few-shot prompt, there is no change in the system message compared with the zero-shot prompt. However, we augment this system message with few shot examples.  "
            ],
            "metadata": {
                "id": "rdsQv1Pz7mJE",
                "azdata_cell_guid": "dd9f3cf4-3bb7-42fa-957d-569828087ba7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "few_shot_system_message = \"\"\"\n",
                "Classify the following product desciption presented in the input into one of the following categories.\n",
                "Categories - ['Hair Care', 'Skin Care']\n",
                "Product description will be delimited by triple backticks in the input.\n",
                "Answer only 'Hair Care' or 'Skin Care'. Do not explain your answer.\n",
                "\"\"\""
            ],
            "metadata": {
                "id": "2nrW4jzGLdce",
                "azdata_cell_guid": "1f05caeb-7773-44ef-bc79-4a5df771f0c9",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "To assemble few-shot examples, we will need to sample the required number of reviews from the training data. One approach would be to  first subset the different categories and then select samples from these subsets."
            ],
            "metadata": {
                "id": "RZcz4TtHQPa2",
                "azdata_cell_guid": "71843e6d-2dc1-421c-bed2-afba3e19280d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "hc_reviews = (examples_df.Category == 'Hair Care')\n",
                "sc_reviews = (examples_df.Category == 'Skin Care')"
            ],
            "metadata": {
                "id": "VJjyRP5IQAx4",
                "azdata_cell_guid": "ed2e4bd4-e78e-4c92-ba81-f4d8b038861a",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "hc_examples = examples_df.loc[hc_reviews, :].sample(4)\n",
                "sc_examples = examples_df.loc[sc_reviews, :].sample(4)"
            ],
            "metadata": {
                "id": "cT3eeB0vPahs",
                "azdata_cell_guid": "31f04329-cdca-4fe8-a432-86ad48c1ed31",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "hc_examples"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 175
                },
                "id": "dko5YsMaPahs",
                "outputId": "13dc8993-77ad-446e-fa8a-4beb27147e2f",
                "azdata_cell_guid": "afe69f55-b46c-4917-8558-4523ca319ec9",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "To reiterate from our learnings from the week, merely selecting random samples from the category subsets is not enough because the examples included in a prompt are prone to a set of known biases. LLMs are known to respond with the most frequent label in the examples or the labels that were given at the end of the prompt.\n",
                "\n"
            ],
            "metadata": {
                "id": "d3ExNtgPRDhJ",
                "azdata_cell_guid": "e6466ae1-9e0f-42de-8810-df46b485282b"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "To avoid these biases, it is important to have a balanced set of examples that are arranged in random order. Let us create a Python function that generates bias-free examples (our function implements the workflow presented below):"
            ],
            "metadata": {
                "id": "bEA_Ab2FRrYB",
                "azdata_cell_guid": "39fa746d-2b29-4318-ab47-722a513970da"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def create_examples(dataset, n=4):\n",
                "\n",
                "    \"\"\"\n",
                "    Return a JSON list of randomized examples of size 2n with two classes.\n",
                "    Create subsets of each class, choose random samples from the subsets,\n",
                "    merge and randomize the order of samples in the merged list.\n",
                "    Each run of this function creates a different random sample of examples\n",
                "    chosen from the training data.\n",
                "\n",
                "    Args:\n",
                "        dataset (DataFrame): A DataFrame with examples (text + label)\n",
                "        n (int): number of examples of each class to be selected\n",
                "\n",
                "    Output:\n",
                "        randomized_examples (JSON): A JSON with examples in random order\n",
                "    \"\"\"\n",
                "\n",
                "    hc_reviews = (examples_df.Category == 'Hair Care')\n",
                "    sc_reviews = (examples_df.Category == 'Skin Care')\n",
                "\n",
                "    cols_to_select = [\"Product Description\",\"Category\"]\n",
                "    hc_examples = examples_df.loc[hc_reviews, cols_to_select].sample(n)\n",
                "    sc_examples = examples_df.loc[sc_reviews, cols_to_select].sample(n)\n",
                "\n",
                "    examples = pd.concat([hc_examples,sc_examples])\n",
                "    # sampling without replacement is equivalent to random shuffling\n",
                "    randomized_examples = examples.sample(2*n, replace=False)\n",
                "\n",
                "    return randomized_examples.to_json(orient='records')"
            ],
            "metadata": {
                "id": "Qb0mLtE9R7ZA",
                "azdata_cell_guid": "9212bee9-60ee-40ef-9839-3ad1d2e63f88",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "examples = create_examples(examples_df, 2)"
            ],
            "metadata": {
                "id": "jb1VpSgZPaht",
                "azdata_cell_guid": "caa8908c-ee2c-4854-b5dc-2cb3459e186d",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "json.loads(examples)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "dJInVW2PSucc",
                "outputId": "6684c598-1cb3-48f1-b890-3fb7db83e503",
                "azdata_cell_guid": "09ef852e-ba18-4f6b-aac7-2fcf85660322",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's create a function to create few show prompt from our examples."
            ],
            "metadata": {
                "id": "PEnNkOzJSwTk",
                "azdata_cell_guid": "34f1bebb-6363-4ab0-ab6b-3255aa1f0ca3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def create_prompt(system_message, examples, user_message_template):\n",
                "\n",
                "    \"\"\"\n",
                "    Return a prompt message in the format expected by the Open AI API.\n",
                "    Loop through the examples and parse them as user message and assistant\n",
                "    message.\n",
                "\n",
                "    Args:\n",
                "        system_message (str): system message with instructions for classification\n",
                "        examples (str): JSON string with list of examples\n",
                "        user_message_template (str): string with a placeholder for description\n",
                "\n",
                "    Output:\n",
                "        few_shot_prompt (List): A list of dictionaries in the Open AI prompt format\n",
                "    \"\"\"\n",
                "\n",
                "    few_shot_prompt = [{'role':'system', 'content': system_message}]\n",
                "\n",
                "    for example in json.loads(examples):\n",
                "        example_description = example['Product Description']\n",
                "        example_category = example['Category']\n",
                "\n",
                "        few_shot_prompt.append(\n",
                "            {\n",
                "                'role': 'user',\n",
                "                'content': user_message_template.format(\n",
                "                    product_description=example_description\n",
                "                )\n",
                "            }\n",
                "        )\n",
                "\n",
                "        few_shot_prompt.append(\n",
                "            {'role': 'assistant', 'content': f\"{example_category}\"}\n",
                "        )\n",
                "\n",
                "    return few_shot_prompt"
            ],
            "metadata": {
                "id": "mFK2WQOgS8ey",
                "azdata_cell_guid": "90399798-a2a6-4ca6-bba8-ad8574622506",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "few_shot_prompt = create_prompt(\n",
                "    few_shot_system_message,\n",
                "    examples,\n",
                "    user_message_template\n",
                ")"
            ],
            "metadata": {
                "id": "QoO1lCFtTBUV",
                "azdata_cell_guid": "1ac8121d-6fda-4316-b331-aba6f035483d",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "few_shot_prompt"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "sy91eq9FTNeo",
                "outputId": "00f463bc-7e5d-4437-88e7-58e629701e08",
                "azdata_cell_guid": "cebe3325-6bb6-478c-8eb5-9c3439c1ff12",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "The few-shot prompt is definetely heavier than the zero-shot prompt. Let's check how much more resource intensive few-shot is."
            ],
            "metadata": {
                "id": "_MPSIcmN1XfS",
                "azdata_cell_guid": "6cdb5faa-9f24-4163-bbaf-537ebbea83eb"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "num_tokens_from_messages(few_shot_prompt)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "ItFH6zLJMnb9",
                "outputId": "19775c4a-5cd0-4320-9912-011924a09794",
                "azdata_cell_guid": "c203f99a-3021-45c1-8772-4f7e0afd227f",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "That is 3x more token usage than zero-shot. Unless it gives significatnly better results, zero-shot will be the preferred one."
            ],
            "metadata": {
                "id": "L4iqG_4L1miM",
                "azdata_cell_guid": "6d0fccc7-70f1-400c-9989-d65f55c978b8"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "sp7awSHZVSw2",
                "outputId": "68b05f23-939e-4368-d8e6-6a2ebe5b370a",
                "azdata_cell_guid": "1867ca7f-2af6-4e46-9afc-0fbbf48d3ac1",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's take the final call after running through all prompting techniques and after running the evaluation across multiple samples."
            ],
            "metadata": {
                "id": "IVqpnL5i10Kp",
                "azdata_cell_guid": "0a482234-43c1-462c-9ccc-acbfe5d3d0a7"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Prompt 3: Chain-of-Thought**"
            ],
            "metadata": {
                "id": "agDW1s7eNy-L",
                "azdata_cell_guid": "e8c53f75-0d4d-4530-9ce8-36af3b1f57a6"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "For the CoT prompt, we add detailed step-by-step instructions to the few shot system message instructing the model to carefully ponder before assigning the label. Apart from this addition, there are no further changes from the few-shot prompt."
            ],
            "metadata": {
                "id": "KKSPE0fT52EE",
                "azdata_cell_guid": "818f36ea-36be-4198-ab0c-7eccac5ac0f9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "cot_system_message = \"\"\"\n",
                "Given the following product description, follow these steps to determine the appropriate product label category:\n",
                "\n",
                "1. Read the product description carefully, looking for key words and phrases that indicate the product's purpose and usage.\n",
                "\n",
                "2. Consider if the description mentions any particular keywords relating to a category.\n",
                "\n",
                "3. If the description contains keywords related to multiple categories, determine which category is most strongly emphasized or which usage is primary.\n",
                "\n",
                "4. If the description does not contain any clear keywords related to the given categories, consider the overall context and purpose of the product to make an educated guess about the most appropriate category.\n",
                "\n",
                "5. Output the determined category label ( 'Hair Care', or 'Skin Care') and nothing else. Do not explain your output.\n",
                "\"\"\""
            ],
            "metadata": {
                "id": "MqhNfIrrPaht",
                "azdata_cell_guid": "234c873f-724f-48ff-a667-c424fde540b7",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "cot_few_shot_prompt = create_prompt(cot_system_message, examples, user_message_template)"
            ],
            "metadata": {
                "id": "bUMhbkjaTjmH",
                "azdata_cell_guid": "7df427b6-b4cd-4361-8b07-6d211e46fe7a",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "cot_few_shot_prompt"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "-KroTz4iTscw",
                "outputId": "6b81285b-2227-4960-f408-823db6d37b69",
                "azdata_cell_guid": "06dad028-4260-4639-8e0e-5d8874b47e26",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Note that the examples remain the same while the system message changes."
            ],
            "metadata": {
                "id": "usookJPxTkEd",
                "azdata_cell_guid": "f210df72-a6dd-4f87-8c3c-5915d7366319"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "num_tokens_from_messages(cot_few_shot_prompt)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "193APKHhTvq6",
                "outputId": "f3ddf4b8-a868-47cc-a3ce-1ebd9f6c1d0c",
                "azdata_cell_guid": "0a26469c-cf5d-4feb-8011-7a4d995f6476",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "We can see that token consumption per example is highest in cot_fewshot followed by fewshot and the least by zero-shot. As the business has to process a lot of products, we need to make sure the token consumption is low as openAI charges the business per token basis. Even small improvements in the token consumption while keeping the accuracies can have a huge impact."
            ],
            "metadata": {
                "id": "KGA93m50MTKq",
                "azdata_cell_guid": "3bd560d9-fa13-4873-91b6-9d0f143c53f0"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's check the cot-fewshot prompt and see if it's worth the token it is consuming."
            ],
            "metadata": {
                "id": "IqmO_biiNZd-",
                "azdata_cell_guid": "23fbd367-3a26-4e54-a9c7-653b37087186"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "evaluate_prompt(cot_few_shot_prompt, gold_examples, user_message_template)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "QgzKAGQYNWar",
                "outputId": "e6028002-d4dc-4198-f77d-7b52f575ee28",
                "azdata_cell_guid": "646f5eb1-d3c5-4cf8-a031-28a0c10357ec",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "We have done evaluations of all three pormpting techniques. Now, let's sample different examples for the few-shot and CoT-few-shot prompts and evaluate them across multiple samples."
            ],
            "metadata": {
                "id": "vBh9D96BVRjb",
                "azdata_cell_guid": "670b2ad2-9f3b-4161-bb7b-353807d59f5d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "num_eval_runs = 5"
            ],
            "metadata": {
                "id": "GTXUDXYy7dku",
                "azdata_cell_guid": "c31c89c9-e34e-4021-94a8-355c2f6c29a6",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "few_shot_performance, cot_few_shot_performance = [], []"
            ],
            "metadata": {
                "id": "n6pQkocytqG_",
                "azdata_cell_guid": "912b3b3d-cce7-44bd-a2aa-c2c212032e8e",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "for _ in tqdm(range(num_eval_runs)):\n",
                "\n",
                "    # For each run create a new sample of examples\n",
                "    examples = create_examples(examples_df)\n",
                "\n",
                "    # Assemble the few shot prompt with these examples\n",
                "    few_shot_prompt = create_prompt(few_shot_system_message, examples, user_message_template)\n",
                "    cot_few_shot_prompt = create_prompt(cot_system_message, examples, user_message_template)\n",
                "\n",
                "    # Evaluate prompt accuracy on gold examples\n",
                "    few_shot_accuracy = evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)\n",
                "    cot_few_shot_accuracy = evaluate_prompt(cot_few_shot_prompt, gold_examples, user_message_template)\n",
                "\n",
                "    few_shot_performance.append(few_shot_accuracy)\n",
                "    cot_few_shot_performance.append(cot_few_shot_accuracy)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "BdUaodX3tseE",
                "outputId": "b81bca36-b626-4a2d-febe-e6ccff529046",
                "azdata_cell_guid": "20f9396f-510d-4e0e-a518-0b5e4c663a35",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "np.array(few_shot_performance).mean(), np.array(few_shot_performance).std()"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "ZikcR8oGrWls",
                "outputId": "224fcd28-4621-47cc-ce7a-89b978232609",
                "azdata_cell_guid": "d62bc059-4c9e-4f00-9aa3-64d0df1b5bd0",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "np.array(cot_few_shot_performance).mean(), np.array(cot_few_shot_performance).std()"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "D-GYUYGkuQY7",
                "outputId": "d83be751-b48c-401d-bfa6-ace10f49eb4b",
                "azdata_cell_guid": "6246dfa9-1543-459e-adb1-f848ac29f9c8",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Both of them are consistent across iterations."
            ],
            "metadata": {
                "azdata_cell_guid": "b4c6bf62-1f09-46e0-8cd6-b2b77ce158e7"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "All of them beat the existing mis-labeling rate. We can see that both zero-shot and few-shot have out performed cot-fewshot. It is imperative for us to use zero-shot over few-shot as the accuracy scores are similar but zero-shot consumes 3X lesser tokens and hence becomes the obvious choice."
            ],
            "metadata": {
                "id": "txhTHxZjVoGg",
                "azdata_cell_guid": "51cce8f5-ea39-46e2-8249-7a53b14e04e7"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Task 2: Intent Detection"
            ],
            "metadata": {
                "id": "leLS3CKbPahz",
                "azdata_cell_guid": "ad62461b-bbb7-449e-9ac8-439f1b290603"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's proceed to the second task, which involves determining customer intent from support queries. We have received a raw dataset of these queries and are required to conduct a preliminary analysis. Our goal is to identify various categories of inquiries and determine the most frequent ones. Based on this analysis, management will establish a labeling team to manually classify the queries into the top three categories, with a fourth category, 'Others,' for queries that do not fit into the top three.\n",
                "\n",
                "Following this step, we will undertake a classification task using OpenAI's tools. If the results are satisfactory (i.e., accuracy greater than 85%), we will apply the model to the entire dataset to determine the actual frequency of each category. The category with the highest occurrence will be prioritized for action.\n",
                "\n",
                "During this session, our focus will be on the initial exploration, construction, and evaluation of the classification task using a large language model (LLM)."
            ],
            "metadata": {
                "id": "3J23ZILoH_Wi",
                "azdata_cell_guid": "487ce554-adb3-437a-9011-9c1960699e73"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Step 2: Assemble and Explore Data"
            ],
            "metadata": {
                "id": "bdzr238VBhpN",
                "azdata_cell_guid": "004382d5-5cd4-4bae-b172-7cf931979086"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "dataset = pd.read_csv(\"customer_intent_raw.csv\")"
            ],
            "metadata": {
                "id": "JWtAqDRdiZS-",
                "azdata_cell_guid": "cc6087b5-7d4a-4f3d-b8e0-a1c8aaab3ae3",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "dataset.head()"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 206
                },
                "id": "u2Ywm4EGikOs",
                "outputId": "94229e18-6d89-4ffd-98f9-2a1557fbbd86",
                "azdata_cell_guid": "0c547dbd-1299-4248-8fbe-cba1bbed6e97",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "dataset.shape"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "h7fwJzUKd8cX",
                "outputId": "19ca73f2-ae7c-4b26-b289-6469aababe54",
                "azdata_cell_guid": "1bcbb9a1-a962-4bde-b424-e4b15ee495fe",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "We are given an unlabeled dataset. First, let's try to understand what sort of problems are present in the data and which of them are most frequent using an LLM."
            ],
            "metadata": {
                "id": "yW0MRthfal3-",
                "azdata_cell_guid": "53040233-99be-4dd4-9cbd-9fa93a2c9b6c"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's concatenate all the responses into one string and then pass it to the LLM and ask it to provide us this preliminary analysis."
            ],
            "metadata": {
                "id": "NUwCKi5FbWRQ",
                "azdata_cell_guid": "ac4e6f28-169a-4faf-8386-67da97e5a0a8"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "raw_intent = dataset['description'].str.cat(sep=' || ')"
            ],
            "metadata": {
                "id": "BHLpnzbxbhfE",
                "azdata_cell_guid": "f1472585-17f9-42ed-a1ae-61b8815ae57c",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "raw_intent"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 140
                },
                "id": "iEo8lBXsc2JE",
                "outputId": "534d3015-d000-44d3-9236-5ce2988ff871",
                "azdata_cell_guid": "3d1c00cb-0bfa-43bf-894a-d5bf697839eb",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let us now craft a prompt."
            ],
            "metadata": {
                "id": "M5srXGNWb6v5",
                "azdata_cell_guid": "1528dca9-887a-4e55-a4d9-0bd96719b331"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "prompt = f\"\"\"Extract the list of problems faced by the user from the following customer queries, separated by \" || \". A problem category should be a two word label. List out 10 unique problems\n",
                "Then, identify the top 5 most frequent problem encountered by the user.\n",
                "\n",
                "Customer queries:\n",
                "{raw_intent}\"\"\""
            ],
            "metadata": {
                "id": "Hgfkh6CrdUMC",
                "azdata_cell_guid": "f87df832-9bb0-44d7-a8e0-61b5d8d57654",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "messages = [{'role':'user','content':prompt}]"
            ],
            "metadata": {
                "id": "oRP_OrxJduG1",
                "azdata_cell_guid": "e8e14e8d-caeb-4a61-a4fa-4771a83861b9",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "response = client.chat.completions.create(\n",
                "    model=deployment_name,\n",
                "    messages=messages,\n",
                "    temperature = 0,\n",
                "    seed = 49\n",
                ")\n",
                "\n",
                "print(response.choices[0].message.content)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "255GvuV3alEt",
                "outputId": "ef34f7a3-f5be-4133-d992-f157aacd8147",
                "azdata_cell_guid": "382b4c38-e8ec-4c86-a0f8-7d3b5b783d30",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "The response from the LLM will vary every time we re-generate it. Since we are doing an unsupervised task and since the LLM has no concrete idea about the problem beforehand, it is expected that there will be some variance. We can iterate over this multiple times and check the outputs each times to figure out the most consistent responses. By doing that, we have a higher chance of actually finding out which of the labels is most frequent. Ideally this should be done on the whole data corpus. Each time sampling a different subset (so that we can fit the context length of the LLM). But in this case-study, we do not have the whole datset (thousands of customer queries). Hence, we don't need to sample the dataset. Instead, we can run the LLM through the data multiple times and find out the most frequent query category (most probably)."
            ],
            "metadata": {
                "id": "exBh5iTG0Eb7",
                "azdata_cell_guid": "db0a95bd-712c-4fbc-9a4f-12c9f8719da0"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "You are encouraged to re-run the above cell multiple times (4-5) to check which are the most consistent labels. Find 2-3 labels that are most consistent in those runs. From our runs, we found the following labels consistent - Modifying order(change order), track order, payment issues, account related issues. Of these three labels, modifying orders, payment issues and lastly track order are the most important ones to fix as they are directly related to revenue. Account related issues is a second priority. Let's look at change order, track oder and payment issues can be further investigated at large scale to understand which of these three is most problematic.\n",
                "\n",
                "The whole classification task would have been a bigger project than actually correcting the three issues if it weren't for LLMs. The NLP classification itself would have taken multiple weeks. However, with LLMs, the whole task can be done within a week."
            ],
            "metadata": {
                "id": "p2HznbCpeup0",
                "azdata_cell_guid": "f077181d-b86c-4469-beb1-aa621964af63"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Using this preliminary exploration, a labeling team has been setup to categorize a sample of the data into the following categories - 'track order', 'change order', 'payment issues', and 'others'."
            ],
            "metadata": {
                "id": "lBB2Q_TneJPc",
                "azdata_cell_guid": "d717501a-8779-42b6-96e4-b795c5421c6c"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Note: We recommend you use the labeled data provided even if your analysis has resulted in a different set of frequent categories (it is a rare chance). Since we have a synthetic data on our hands currently, and since the dataset is rather small, we have to make such choices."
            ],
            "metadata": {
                "id": "NsVHJ5A_unK9",
                "azdata_cell_guid": "445ae552-e815-4247-8699-2edccd9b3a19"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's import the labeled data."
            ],
            "metadata": {
                "id": "7d33cYejtsFS",
                "azdata_cell_guid": "d1dcd78d-c176-42e5-b1a6-651c8137a37a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "dataset_df = pd.read_csv(\"customer_intent_labeled.csv\")"
            ],
            "metadata": {
                "id": "D-oEU6RSeMil",
                "azdata_cell_guid": "1e974aa5-82f6-4a65-926f-814b2507b06a",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "dataset_df.task.unique()"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Df4YwD7WeXLU",
                "outputId": "af6400d6-f421-44a4-912d-592e469e3f57",
                "azdata_cell_guid": "c7e155f0-bf88-4fb5-b038-0073e2f4a6c7",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's check the different categories of customer inquiries."
            ],
            "metadata": {
                "id": "QH0UiBqD_riw",
                "azdata_cell_guid": "ec514846-db64-4875-b700-6e765a2de251"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "examples_df, gold_examples_df = train_test_split(\n",
                "    dataset_df, #<- the full dataset\n",
                "    test_size=0.6, #<- 60% random sample selected for gold examples as we will only need few examples for few-shot\n",
                "    random_state=42 #<- ensures that the splits are the same for every session\n",
                ")"
            ],
            "metadata": {
                "id": "t9Qh5qHYaxG_",
                "azdata_cell_guid": "b4b02e0b-379b-443c-a3b1-b4c43c479f70",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's have a feel of the data before we move forward."
            ],
            "metadata": {
                "id": "0OBQrTPk2JHH",
                "azdata_cell_guid": "839abe56-b2f9-4703-a0de-49905a85506a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "examples_df.sample(10)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 363
                },
                "id": "7mWJSPJ5_rWk",
                "outputId": "43f40ca7-5418-4312-dac1-3d11813581b8",
                "azdata_cell_guid": "f4ab89db-40dd-4eb8-a4a6-94e1b46363a1",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "gold_examples = json.loads((gold_examples_df.sample(100, random_state=42).to_json(orient='records')\n",
                "))"
            ],
            "metadata": {
                "id": "m1Z5EjrzLz6b",
                "azdata_cell_guid": "92cce40b-77b5-4349-ab28-776636b8828f",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's derive prompts for this scenario."
            ],
            "metadata": {
                "id": "OCTPfG6iJr1L",
                "azdata_cell_guid": "f2e2742d-daaf-4ca0-91a7-332fdd6b724f"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Step 3: Derive Prompt"
            ],
            "metadata": {
                "id": "b0u9bqrrDutk",
                "azdata_cell_guid": "12315b85-d587-4e59-8cdd-241fc13a018f"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Create prompts"
            ],
            "metadata": {
                "id": "sunRoyDMMbBu",
                "azdata_cell_guid": "f132e9f3-db61-4b28-8ccf-c7ec6b2295f9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "user_message_template = \"\"\"```{description}```\"\"\""
            ],
            "metadata": {
                "id": "Kx-5s0g_LZKM",
                "azdata_cell_guid": "46065eaa-1b4a-4e56-af57-423fe75e2dcc",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Zero-shot prompt**"
            ],
            "metadata": {
                "id": "GjvavZ2moPkm",
                "azdata_cell_guid": "9d22fc8f-5fd2-49f8-9705-8599124e21ab"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's craft our zero-shot such that the model doesn't respond with an explanation or output a new label that's not one of our pre-determined labels."
            ],
            "metadata": {
                "id": "JtbYpD84g4Ff",
                "azdata_cell_guid": "40c740c1-d344-4db9-89a1-54e68fdfe60c"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "zero_shot_system_message = \"\"\"Classify the following product desciption presented in the input into one of the following categories.\n",
                "Categories - ['change order', 'track order', 'payment issue', 'others']\n",
                "Product description will be delimited by triple backticks in the input.\n",
                "Answer only 'change order',\n",
                " or 'track order', or 'payment issue',or 'others'. Nothing Else. Do not explain your answer.\n",
                "\"\"\""
            ],
            "metadata": {
                "id": "wKW9dlJuEewo",
                "azdata_cell_guid": "4deceb27-0495-47e9-ad5e-6a642544cb6f",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "zero_shot_prompt = [{'role':'system', 'content': zero_shot_system_message}]"
            ],
            "metadata": {
                "id": "_5SIubZXLhwM",
                "azdata_cell_guid": "f818818c-30d8-4b7a-b8e3-846a112b5b96",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "num_tokens_from_messages(zero_shot_prompt)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "91PlM6xILifx",
                "outputId": "82319adb-daf6-4a22-fdbf-e1dadfc1cbba",
                "azdata_cell_guid": "2e6eee8f-3643-4318-ad7d-cb9b9acd2e25",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Few-shot prompt**"
            ],
            "metadata": {
                "id": "3Ar7GVoYoXWz",
                "azdata_cell_guid": "3bf126cd-6b9d-40ba-ae8c-7637d89d6a58"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's also create a few-shot prompt. We can evaluate both zero-shot and few-shot at the same time later."
            ],
            "metadata": {
                "id": "XQ8_s8Ng1_nC",
                "azdata_cell_guid": "36e43097-2ed1-46c7-9b89-a56452d1f522"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "few_shot_system_message = \"\"\"Classify the following product desciption presented in the input into one of the following categories.\n",
                "Categories - ['change order', 'track order', 'payment issue']\n",
                "Product description will be delimited by triple backticks in the input.\n",
                "Answer only 'change order', or 'track order', or 'payment issue'. Nothing Else. Do not explain your answer.\n",
                "\"\"\""
            ],
            "metadata": {
                "id": "Nw06_SbIoagf",
                "azdata_cell_guid": "711c24a4-955d-4f80-aa5b-7de43517e2cf",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "def create_examples(dataset, n=4):\n",
                "\n",
                "    \"\"\"\n",
                "    Return a JSON list of randomized examples of size 2n with two classes.\n",
                "    Create subsets of each class, choose random samples from the subsets,\n",
                "    merge and randomize the order of samples in the merged list.\n",
                "    Each run of this function creates a different random sample of examples\n",
                "    chosen from the training data.\n",
                "\n",
                "    Args:\n",
                "        dataset (DataFrame): A DataFrame with examples (text + label)\n",
                "        n (int): number of examples of each class to be selected\n",
                "\n",
                "    Output:\n",
                "        randomized_examples (JSON): A JSON with examples in random order\n",
                "    \"\"\"\n",
                "\n",
                "    task1 = (examples_df.task == 'change order')\n",
                "    task2 = (examples_df.task == 'track order')\n",
                "    task3 = (examples_df.task == 'payment issue')\n",
                "    task4 = (examples_df.task == 'others')\n",
                "\n",
                "    t1_examples = examples_df.loc[task1, :].sample(n)\n",
                "    t2_examples = examples_df.loc[task2, :].sample(n)\n",
                "    t3_examples = examples_df.loc[task3, :].sample(n)\n",
                "    t4_examples = examples_df.loc[task4, :].sample(n)\n",
                "    examples = pd.concat([t1_examples,t2_examples,t3_examples,t4_examples])\n",
                "    # sampling without replacement is equivalent to random shuffling\n",
                "    randomized_examples = examples.sample(4*n, replace=False)\n",
                "\n",
                "    return randomized_examples.to_json(orient='records')"
            ],
            "metadata": {
                "id": "u7vL5Unzomo_",
                "azdata_cell_guid": "9ca48c19-72ec-48e3-ab92-e173b591ea44",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "def create_prompt(system_message, examples, user_message_template):\n",
                "\n",
                "    \"\"\"\n",
                "    Return a prompt message in the format expected by the Open AI API.\n",
                "    Loop through the examples and parse them as user message and assistant\n",
                "    message.\n",
                "\n",
                "    Args:\n",
                "        system_message (str): Instructions for the model for classfication\n",
                "        examples (JSON): JSON list of examples representative of each category\n",
                "        user_message_template (str): string with a placeholder for query\n",
                "\n",
                "    Output:\n",
                "        few_shot_prompt (List): A list of dictionaries in the Open AI prompt format\n",
                "    \"\"\"\n",
                "\n",
                "    few_shot_prompt = [{'role':'system', 'content': system_message}]\n",
                "\n",
                "    for example in json.loads(examples):\n",
                "\n",
                "        few_shot_prompt.append(\n",
                "            {\n",
                "                'role': 'user',\n",
                "                'content': user_message_template.format(\n",
                "                    description=example['description']\n",
                "                )\n",
                "            }\n",
                "        )\n",
                "\n",
                "        few_shot_prompt.append(\n",
                "            {'role': 'assistant', 'content': f\"{example['task']}\"}\n",
                "        )\n",
                "\n",
                "    return few_shot_prompt"
            ],
            "metadata": {
                "id": "jMsCe7VYqKM1",
                "azdata_cell_guid": "1842634c-a2ab-4c7b-a772-68e38a41f05d",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "examples = create_examples(examples_df,2)\n",
                "gold_examples = create_examples(gold_examples_df,10)\n",
                "few_shot_prompt = create_prompt(few_shot_system_message, examples, user_message_template)"
            ],
            "metadata": {
                "id": "8X4wsf5VqqOg",
                "azdata_cell_guid": "580a2ce8-8c92-4e56-a33a-9e7fe123e758",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "json.loads(gold_examples)[0]"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "MkbMhgUqiSlt",
                "outputId": "52840c68-2672-4b31-eaa2-3c68c41ab8b5",
                "azdata_cell_guid": "677f89a4-4f11-4db2-9942-33f6561a1775",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's check the few shot prompt."
            ],
            "metadata": {
                "id": "Q8NRyBTUxRPL",
                "azdata_cell_guid": "6aac0738-68e1-46f7-9aaa-b232b20da691"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "few_shot_prompt"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "YREwWcXWWkZa",
                "outputId": "bc21f654-059f-407f-8c94-555557b053e7",
                "azdata_cell_guid": "900715de-22eb-4a9c-846e-14e7fe62175f",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Evaluate prompts"
            ],
            "metadata": {
                "id": "zaOqdj8hMmnZ",
                "azdata_cell_guid": "446d35d9-39b1-4609-a62b-e5a29ad4fe9d"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's evaluate our zero-shot and few-shot prompts."
            ],
            "metadata": {
                "id": "fEMKfieozF0P",
                "azdata_cell_guid": "9589bfbc-cf84-40cf-b6b9-9e98a2421b5b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def evaluate_prompt(prompt, gold_examples, user_message_template, samples_to_output = 10):\n",
                "\n",
                "    \"\"\"\n",
                "    Return the accuracy for predictions on gold examples.\n",
                "    For each example, we make a prediction using the prompt. Gold labels and\n",
                "    model predictions are aggregated into lists and compared to compute the\n",
                "    accuracy.\n",
                "\n",
                "    Args:\n",
                "        prompt (List): list of messages in the Open AI prompt format\n",
                "        gold_examples (str): JSON string with list of gold examples\n",
                "        user_message_template (str): string with a placeholder for description\n",
                "        samples_to_output (int): number of sample predictions and ground truths to print\n",
                "\n",
                "    Output:\n",
                "        accuracy (float): accuracy score computed by comparing model predictions\n",
                "                                with ground truth\n",
                "    \"\"\"\n",
                "    count = 0\n",
                "    model_predictions, ground_truths = [], []\n",
                "\n",
                "    # Iterating through all the gold examples and constructing the messages dictionary using the text from example\n",
                "\n",
                "    for example in json.loads(gold_examples):\n",
                "        gold_input = example['description']\n",
                "        user_input = [\n",
                "            {\n",
                "                'role':'user',\n",
                "                'content': user_message_template.format(description=gold_input)\n",
                "            }\n",
                "        ]\n",
                "\n",
                "        try:\n",
                "            response = client.chat.completions.create(\n",
                "                model=deployment_name,\n",
                "                messages=prompt+user_input,\n",
                "                temperature=0, # <- Note the low temperature\n",
                "                max_tokens=4 # <- Note how we restrict the output to not more than 2 tokens\n",
                "            )\n",
                "\n",
                "            prediction = response.choices[0].message.content\n",
                "            model_predictions.append(prediction.strip().lower()) # <- removes extraneous white space and lowercases output\n",
                "            ground_truths.append(example['task'].strip().lower())\n",
                "\n",
                "            if count < samples_to_output:\n",
                "              count += 1\n",
                "              print(\"Original label: \\n\", example['task'],\"\\n\")\n",
                "              print(\"Predicted label: \\n\", prediction)\n",
                "              print(\"================================\")\n",
                "\n",
                "        except Exception as e:\n",
                "            print(e)\n",
                "            continue\n",
                "\n",
                "\n",
                "    accuracy = accuracy_score(ground_truths, model_predictions)\n",
                "\n",
                "    return accuracy"
            ],
            "metadata": {
                "id": "1ZOXJpIGMoc-",
                "azdata_cell_guid": "0f6cfdfc-f3d0-4987-b7d3-4db52ed736bd",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "evaluate_prompt(zero_shot_prompt, gold_examples, user_message_template)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Ehpfuk8ZN0ed",
                "outputId": "d51b26f7-be4a-4b30-d508-d9bb76fd7054",
                "azdata_cell_guid": "ac121089-bfae-45c7-943e-49d7109c482c",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Hxli5e04rNaM",
                "outputId": "b4d1b9d8-8df0-4574-a4cd-b27546b485fd",
                "azdata_cell_guid": "f180ab50-e031-45d0-9839-d12b804404fc",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "That is a good result! Let's see if it is consistent across multiple samples."
            ],
            "metadata": {
                "id": "g3iYvr8ZKH4b",
                "azdata_cell_guid": "15f577df-b075-4943-a69c-7030608a1e05"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "num_eval_runs = 5"
            ],
            "metadata": {
                "id": "SB2aVW4ht80U",
                "azdata_cell_guid": "617b77a5-cd2c-4548-9cb8-5fd7e668dd1f",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "few_shot_performance = []"
            ],
            "metadata": {
                "id": "dhwA3hMlt80f",
                "azdata_cell_guid": "79ee80db-5212-45de-8766-a2e2e823aad9",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's modify the original evaluate function in such a way that it prints the accuracy for every ground truth label and the overall accuracy for every round of evaluation."
            ],
            "metadata": {
                "id": "1q-wlEuOzsV1",
                "azdata_cell_guid": "326d78b7-2ede-4478-9c80-dcaffa5b2144"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def evaluate_prompt(prompt, gold_examples, user_message_template,samples_to_output = 10):\n",
                "\n",
                "    \"\"\"\n",
                "    Return the accuracy score for predictions on gold examples.\n",
                "    For each example, we make a prediction using the prompt. Gold labels and\n",
                "    model predictions are aggregated into lists and compared to compute the\n",
                "    accuracy.\n",
                "\n",
                "    Args:\n",
                "        prompt (List): list of messages in the Open AI prompt format\n",
                "        gold_examples (str): JSON string with list of gold examples\n",
                "        user_message_template (str): string with a placeholder for description.\n",
                "        samples_to_output (int): number of sample predictions and ground truths to print\n",
                "\n",
                "    Output:\n",
                "        accuracy (float): accuracy score computed by comparing model predictions\n",
                "                                with ground truth\n",
                "    \"\"\"\n",
                "    count = 0\n",
                "    model_predictions, ground_truths = [], []\n",
                "\n",
                "    # Iterating through all the gold examples and constructing the messages dictionary using the text from example\n",
                "\n",
                "    for example in json.loads(gold_examples):\n",
                "        gold_input = example['description']\n",
                "        user_input = [\n",
                "            {\n",
                "                'role':'user',\n",
                "                'content': user_message_template.format(description=gold_input)\n",
                "            }\n",
                "        ]\n",
                "\n",
                "        try:\n",
                "            response = client.chat.completions.create(\n",
                "                model=deployment_name,\n",
                "                messages=prompt+user_input,\n",
                "                temperature=0, # <- Note the low temperature\n",
                "                max_tokens=4 # <- Note how we restrict the output to not more than 2 tokens\n",
                "            )\n",
                "\n",
                "            prediction = response.choices[0].message.content\n",
                "            while count < samples_to_output:\n",
                "              count += 1\n",
                "              print(\"Original label: \\n\", example['task'],\"\\n\")\n",
                "              print(\"Predicted label: \\n\", prediction)\n",
                "\n",
                "\n",
                "            model_predictions.append(prediction.strip().lower()) # <- removes extraneous white space and lowercases output\n",
                "            ground_truths.append(example['task'].strip().lower())\n",
                "\n",
                "        except Exception as e:\n",
                "            print(e)\n",
                "            continue\n",
                "\n",
                "\n",
                "    # Find the accuracy of each category.\n",
                "\n",
                "    df = pd.DataFrame({\n",
                "    'Predictions': model_predictions,\n",
                "    'Ground Truth': ground_truths\n",
                "    })\n",
                "    labels = df['Ground Truth'].unique()\n",
                "\n",
                "    # Create a new DataFrame for the accuracies\n",
                "    accuracy_df = pd.DataFrame(columns=labels)\n",
                "\n",
                "    for label in labels:\n",
                "        # Filter rows where Ground Truth is the current label\n",
                "        subset = df[df['Ground Truth'] == label]\n",
                "\n",
                "        # Calculate accuracy for the current label\n",
                "        accuracy = accuracy_score(subset['Ground Truth'], subset['Predictions'])\n",
                "\n",
                "        # Add accuracy to the DataFrame\n",
                "        accuracy_df.loc[0, label] = accuracy\n",
                "\n",
                "    print(\"\\n\\n\", accuracy_df)\n",
                "    print(\"====================================================\")\n",
                "\n",
                "    accuracy = accuracy_score(ground_truths, model_predictions)\n",
                "\n",
                "    return accuracy"
            ],
            "metadata": {
                "id": "N5aoIO1_zGh_",
                "azdata_cell_guid": "d91d52b5-b9d8-4f9f-9502-351d24065cdb",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "for _ in tqdm(range(num_eval_runs)):\n",
                "\n",
                "    # For each run create a new sample of examples\n",
                "    examples = create_examples(dataset_df)\n",
                "\n",
                "    # Assemble the few shot prompt with these examples\n",
                "    few_shot_prompt = create_prompt(few_shot_system_message, examples, user_message_template)\n",
                "\n",
                "    # Evaluate prompt accuracy on gold examples\n",
                "    few_shot_accuracy = evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)\n",
                "\n",
                "    few_shot_performance.append(few_shot_accuracy)\n"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "GAbDg7tRt80f",
                "outputId": "dacfc4ed-01b0-47d3-e599-4079fb521c1e",
                "azdata_cell_guid": "2397fc17-a87c-4ec6-a815-52403dcf5085",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's check the average accuracy across runs"
            ],
            "metadata": {
                "id": "xKAufBsy_Dhw",
                "azdata_cell_guid": "7d9e8c9a-daa1-4b56-a52b-c3f1c8790962"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "np.array(few_shot_performance).mean(), np.array(few_shot_performance).std()"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "0ti6Y2kAt80f",
                "outputId": "c91bb2f6-f10d-4088-b204-5b34332c362c",
                "azdata_cell_guid": "cc5eb043-8d77-47bb-a8d7-164abe8a8441",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "We got a good result with few-shot technique across multiple samples."
            ],
            "metadata": {
                "id": "cuBvJYn2meJa",
                "azdata_cell_guid": "04f9d712-ff4d-4d4d-8352-f2c02581e315"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "We can observe that most of the mislabeling is happening the specific case where the original label was others and then the LLM instead of placing it in others is misclassifying it into the other categories. Majority of the inaccuracy is thus from this case. This particular case is not that troubling for the business. The opposite of this, which is classifying a label that is not labelled as others into others will mean that an important issue has been mislabeled into a non-major label."
            ],
            "metadata": {
                "id": "XUSpbECim02x",
                "azdata_cell_guid": "71d39129-52db-4acd-960d-ff49511db393"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "We can also manually check these missed cases to analyze this further."
            ],
            "metadata": {
                "id": "gOspY85t1UZ1",
                "azdata_cell_guid": "755951d2-711c-4887-8f51-25475d27803b"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's modify the evaluate prompt so that it prints the query along with the ground truth label and prediction. This will help us see where the LLM is getting it wrong."
            ],
            "metadata": {
                "id": "v7IDjb6a1ja2",
                "azdata_cell_guid": "5134d941-7087-4177-b5d6-f7e64e184bc6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def evaluate_prompt(prompt, gold_examples, user_message_template,samples_to_output = 10):\n",
                "\n",
                "    \"\"\"\n",
                "    Return the accuracy score for predictions on gold examples.\n",
                "    For each example, we make a prediction using the prompt. Gold labels and\n",
                "    model predictions are aggregated into lists and compared to compute the\n",
                "    accuracy.\n",
                "\n",
                "    Args:\n",
                "        prompt (List): list of messages in the Open AI prompt format\n",
                "        gold_examples (str): JSON string with list of gold examples\n",
                "        user_message_template (str): string with a placeholder for description\n",
                "        samples_to_output (int): number of sample predictions and ground truths to print\n",
                "\n",
                "    Output:\n",
                "        accuracy (float): accuracy score computed by comparing model predictions\n",
                "                                with ground truth\n",
                "    \"\"\"\n",
                "    count = 0\n",
                "    model_predictions, ground_truths = [], []\n",
                "\n",
                "    # Iterating through all the gold examples and constructing the messages dictionary using the text from example\n",
                "\n",
                "    for example in json.loads(gold_examples):\n",
                "        gold_input = example['description']\n",
                "        user_input = [\n",
                "            {\n",
                "                'role':'user',\n",
                "                'content': user_message_template.format(description=gold_input)\n",
                "            }\n",
                "        ]\n",
                "\n",
                "        try:\n",
                "            response = client.chat.completions.create(\n",
                "                model=deployment_name,\n",
                "                messages=prompt+user_input,\n",
                "                temperature=0, # <- Note the low temperature\n",
                "                max_tokens=4 # <- Note how we restrict the output to not more than 2 tokens\n",
                "            )\n",
                "\n",
                "            prediction = response.choices[0].message.content\n",
                "            while count < samples_to_output:\n",
                "              count += 1\n",
                "              print(\"Description: \\n\", example['description'],\"\\n\")\n",
                "              print(\"Original label: \\n\", example['task'],\"\\n\")\n",
                "              print(\"Predicted label: \\n\", prediction)\n",
                "              print(\"====================================================\")\n",
                "            model_predictions.append(prediction.strip().lower()) # <- removes extraneous white space and lowercases output\n",
                "            ground_truths.append(example['task'].strip().lower())\n",
                "\n",
                "        except Exception as e:\n",
                "            print(e)\n",
                "            continue\n",
                "\n",
                "\n",
                "    # Find the accuracy of each category.\n",
                "\n",
                "    df = pd.DataFrame({\n",
                "    'Predictions': model_predictions,\n",
                "    'Ground Truth': ground_truths\n",
                "    })\n",
                "    labels = df['Ground Truth'].unique()\n",
                "\n",
                "    # Create a new DataFrame for the accuracies\n",
                "    accuracy_df = pd.DataFrame(columns=labels)\n",
                "\n",
                "    for label in labels:\n",
                "        # Filter rows where Ground Truth is the current label\n",
                "        subset = df[df['Ground Truth'] == label]\n",
                "\n",
                "        # Calculate accuracy for the current label\n",
                "        accuracy = accuracy_score(subset['Ground Truth'], subset['Predictions'])\n",
                "\n",
                "        # Add accuracy to the DataFrame\n",
                "        accuracy_df.loc[0, label] = accuracy\n",
                "\n",
                "    print(\"\\n\\n\", accuracy_df,\"\\n\\n\")\n",
                "\n",
                "    accuracy = accuracy_score(ground_truths, model_predictions)\n",
                "\n",
                "    return accuracy"
            ],
            "metadata": {
                "id": "N8x89Ovv1fP_",
                "azdata_cell_guid": "42fa88e3-4993-415c-8cec-77d887f19ae6",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Ak47_EKf2DbD",
                "outputId": "1d9440a8-4302-4f73-9788-266b57d61b95",
                "azdata_cell_guid": "0586454e-c036-47cf-9691-61ae20c47af0",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "If you run this with multiple samples, you see that the mislabelling is sometimes due to mislabelling during the manual labeling process and the LLM is actually getting it right and the problem is with the manual labeling itself. In cases where it's not due to manual mislabeling, it is an edge case where the label could have gone either way. Overall our analysis shows that the LLM model is a good proxy and can be used to find the most troubling UX experience."
            ],
            "metadata": {
                "id": "O89cBSEZ2XHd",
                "azdata_cell_guid": "bbf9492b-a2d0-4091-a1b0-42d2a6afd09c"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "We can go ahead and use the model to segregate user queries. Post this step, a dashboard can be prepared to show the most frequent labels we have found. This will reveal the most frequent problem encountered by the users."
            ],
            "metadata": {
                "id": "kqxYizhOmUgP",
                "azdata_cell_guid": "13207b0e-d4c5-4493-8f19-e96a2534834f"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Power Ahead!**"
            ],
            "metadata": {
                "id": "mkPgcc5swb89",
                "azdata_cell_guid": "63fa6e1a-4035-488e-9ee6-7988dcf25ccb"
            }
        }
    ]
}